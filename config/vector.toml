# ðŸ¦… NoC Raven - Vector Configuration
# High-performance data pipeline for telemetry processing and forwarding

[api]
enabled = true
address = "0.0.0.0:8084"
playground = false

# Vector logging level is managed via VECTOR_LOG env; no [log] table for compatibility

# =============================================================================
# SOURCES
# =============================================================================

# Syslog input from Fluent Bit
[sources.syslog_input]
type = "syslog"
address = "0.0.0.0:1514"
mode = "udp"
max_length = 102400

# NetFlow data from GoFlow2
[sources.netflow_input]
type = "http"
address = "0.0.0.0:8085"
path = "/netflow"
method = ["POST"]
headers = ["Content-Type"]

# SNMP trap input
[sources.snmp_input]
type = "socket"
address = "0.0.0.0:1162"
mode = "udp"
max_length = 65535
path = "/tmp/noc-raven-snmp.sock"

# Metrics from Telegraf
[sources.metrics_input]
type = "http"
address = "0.0.0.0:8086"
path = "/metrics"
method = ["POST"]
headers = ["Content-Type"]

# Internal Vector metrics
[sources.internal_metrics]
type = "internal_metrics"
scrape_interval_secs = 10

# File input for log files
[sources.log_files]
type = "file"
include = ["/var/log/noc-raven/*.log"]
ignore_older_secs = 600
multiline = { mode = "halt_before", condition_pattern = "^\\d{4}-\\d{2}-\\d{2}" }

# Docker logs (if running in container)
[sources.docker_logs]
type = "docker_logs"
include_labels = ["noc-raven.*"]
partial_event_marker_field = "_partial"

# =============================================================================
# TRANSFORMS
# =============================================================================

# Parse and enrich syslog messages
[transforms.syslog_parser]
type = "remap"
inputs = ["syslog_input"]
source = """
  . = parse_syslog!(.message)
  .source_type = "syslog"
  .appliance = "noc-raven"
  .processed_at = now()
  
  # Extract additional fields if they exist
  if exists(.host) {
    .device_hostname = .host
  }
  
  # Categorize by facility
  .log_category = if .facility == 16 { "local0" }
    else if .facility == 17 { "local1" }
    else { "other" }
  
  # Add severity level name
  .severity_name = if .severity == 0 { "emergency" }
    else if .severity == 1 { "alert" }
    else if .severity == 2 { "critical" }
    else if .severity == 3 { "error" }
    else if .severity == 4 { "warning" }
    else if .severity == 5 { "notice" }
    else if .severity == 6 { "info" }
    else if .severity == 7 { "debug" }
    else { "unknown" }
"""

# Process NetFlow data
[transforms.netflow_processor]
type = "remap"
inputs = ["netflow_input"]
source = """
  . = parse_json!(.message)
  .source_type = "netflow"
  .appliance = "noc-raven"
  .processed_at = now()
  
  # Convert byte values to numbers
  if exists(.bytes) {
    .bytes = to_int(.bytes) ?? 0
  }
  if exists(.packets) {
    .packets = to_int(.packets) ?? 0
  }
  
  # Add flow direction
  .flow_direction = if exists(.src_port) && to_int(.src_port) ?? 0 < 1024 { "inbound" }
    else if exists(.dst_port) && to_int(.dst_port) ?? 0 < 1024 { "outbound" }
    else { "peer-to-peer" }
  
  # Categorize protocols
  .protocol_name = if .protocol == 1 { "icmp" }
    else if .protocol == 6 { "tcp" }
    else if .protocol == 17 { "udp" }
    else { "other" }
"""

# Process SNMP traps
[transforms.snmp_processor]
type = "remap"
inputs = ["snmp_input"]
source = """
  .source_type = "snmp_trap"
  .appliance = "noc-raven"
  .processed_at = now()
  
  # Parse SNMP trap data (basic parsing)
  if contains(string!(.message), "SNMPv2-MIB::sysUpTime") {
    .trap_type = "system"
  } else if contains(string!(.message), "IF-MIB") {
    .trap_type = "interface"
  } else {
    .trap_type = "other"
  }
  
  # Extract community string if present
  if match(string!(.message), r"community: ([\\w-]+)") {
    .community = capture(string!(.message), r"community: ([\\w-]+)")[0]
  }
"""

# Process metrics data
[transforms.metrics_processor]
type = "remap"
inputs = ["metrics_input"]
source = """
  . = parse_json!(.message)
  .source_type = "metrics"
  .appliance = "noc-raven"
  .processed_at = now()
  
  # Add metric categories
  if starts_with(string!(.name), "cpu") {
    .metric_category = "system"
  } else if starts_with(string!(.name), "mem") {
    .metric_category = "system"
  } else if starts_with(string!(.name), "disk") {
    .metric_category = "storage"
  } else if starts_with(string!(.name), "net") {
    .metric_category = "network"
  } else {
    .metric_category = "application"
  }
"""

# Process log files
[transforms.log_processor]
type = "remap"
inputs = ["log_files"]
source = """
  .source_type = "log_file"
  .appliance = "noc-raven"
  .processed_at = now()
  
  # Extract log level from message
  if match(string!(.message), r"\\[(DEBUG|INFO|WARN|ERROR|FATAL)\\]") {
    .log_level = downcase(capture(string!(.message), r"\\[([A-Z]+)\\]")[0])
  } else {
    .log_level = "unknown"
  }
  
  # Extract component from file path
  .component = replace(string!(.file), "/var/log/noc-raven/", "") |> 
               replace(., ".log", "")
"""

# Add geolocation data (if GeoIP database available)
[transforms.geoip_enrichment]
type = "geoip"
inputs = ["netflow_processor", "syslog_parser"]
database = "/usr/share/GeoIP/GeoLite2-City.mmdb"
source = "src_ip"
target = "geo"

# Rate limiting and sampling
[transforms.rate_limiter]
type = "throttle"
inputs = ["syslog_parser", "netflow_processor"]
threshold = 1000
window_secs = 1

# Add tags for routing
[transforms.add_routing_tags]
type = "remap"
inputs = ["syslog_parser", "netflow_processor", "snmp_processor", "metrics_processor"]
source = """
  # Add routing tags based on data type and severity
  if .source_type == "syslog" && (.severity ?? 99) <= 3 {
    .route_priority = "high"
  } else if .source_type == "netflow" && (.bytes ?? 0) > 1000000 {
    .route_priority = "high"
  } else {
    .route_priority = "normal"
  }
  
  # Add customer/tenant information if available
  .tenant = "default"
"""

# =============================================================================
# SINKS
# =============================================================================

# File output for searchable logs (replaces Elasticsearch) - WITH SIZE LIMITS
[sinks.logs_file]
type = "file"
inputs = ["add_routing_tags"]
path = "/data/logs/vector-logs-%Y-%m-%dT%H.log"
compression = "gzip"

[sinks.logs_file.encoding]
codec = "json"

# File output for time-series metrics (replaces InfluxDB) - WITH SIZE LIMITS
[sinks.metrics_file]
type = "file"
inputs = ["metrics_processor", "internal_metrics"]
path = "/data/metrics/vector-metrics-%Y-%m-%dT%H.log"
compression = "gzip"

[sinks.metrics_file.encoding]
codec = "json"

# Local alerts file (replaces webhook) - WITH SIZE LIMITS
[sinks.alerts_file]
type = "file"
inputs = ["add_routing_tags"]
path = "/data/alerts/vector-alerts-%Y-%m-%dT%H.log"
compression = "gzip"

[sinks.alerts_file.encoding]
codec = "json"

# File backup for all data - WITH SIZE LIMITS
[sinks.file_backup]
type = "file"
inputs = ["add_routing_tags"]
path = "/data/buffer/vector-backup-%Y-%m-%dT%H.log"
compression = "gzip"

[sinks.file_backup.encoding]
codec = "json"

# Kafka output disabled for initial deployment
# [sinks.kafka_output]
# type = "kafka"
# inputs = ["add_routing_tags"]
# bootstrap_servers = "localhost:9092"
# topic = "noc-raven-telemetry"
# compression = "gzip"
# acks = "all"
# 
# [sinks.kafka_output.encoding]
# codec = "json"
# timestamp_format = "rfc3339"

# S3 output disabled for initial deployment
# [sinks.s3_archive]
# type = "aws_s3"
# inputs = ["add_routing_tags"]
# bucket = "noc-raven-archive"
# key_prefix = "telemetry/%Y/%m/%d/"
# region = "us-east-1"
# compression = "gzip"
# 
# [sinks.s3_archive.encoding]
# codec = "json"
# timestamp_format = "rfc3339"

# Console output for debugging - DISABLED to prevent log explosion
# [sinks.console_debug]
# type = "console"
# inputs = ["add_routing_tags"]
# target = "stdout"
# 
# [sinks.console_debug.encoding]
# codec = "json"

# Prometheus metrics export
[sinks.prometheus_metrics]
type = "prometheus_exporter"
inputs = ["internal_metrics"]
address = "0.0.0.0:9090"
path = "/metrics"

# =============================================================================
# BUFFERS
# =============================================================================

# Configure disk buffers for reliability - REDUCED SIZES
[sinks.logs_file.buffer]
type = "disk"
max_size = 104857600  # 100MB (reduced from 1GB)
when_full = "drop_newest"

[sinks.metrics_file.buffer]
type = "disk"
max_size = 52428800   # 50MB (reduced from 512MB)
when_full = "drop_newest"

[sinks.file_backup.buffer]
type = "memory"
max_events = 1000     # Reduced from 10000
when_full = "drop_newest"

# =============================================================================
# HEALTH CHECKS
# =============================================================================

[sources.health_check]
type = "http"
address = "0.0.0.0:8087"
path = "/health"
method = ["GET"]

[transforms.health_response]
type = "remap"
inputs = ["health_check"]
source = """
  .status = "healthy"
  .timestamp = now()
  .version = "1.0.0-alpha"
  .uptime = uptime()
"""

[sinks.health_response]
type = "http"
inputs = ["health_response"]
uri = "http://localhost:8087/health"
method = "post"

[sinks.health_response.encoding]
codec = "json"
